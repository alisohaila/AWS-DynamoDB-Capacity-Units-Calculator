{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of N-Gram Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alisohaila/AWS-DynamoDB-Capacity-Units-Calculator/blob/main/Copy_of_N_Gram_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Gram Model\n",
        "An n-gram model generates an original text inspired by existing texts.\n",
        "The main idea behind the n-gram model is to predict the next word in a text based on the previous n words. To build your model, you will need to understand the following terminology:\n",
        "  \n",
        "\n",
        "*   **Corpus**: A collection of texts. Generally, researchers curate corpora using texts with a common theme. For instance, there are corpuses of Reddit comments and Shakespearean plays.\n",
        "*   **Lexicon**: A vocabulary of words and punctuation in a corpus. In the corpus, capitalization in words does not matter. The punctuation is described in the parse_story function. \n",
        "*   **Token**: A single element of the lexicon.\n",
        "*   **N-gram**: A series of N tokens that appear consecutively in the corpus, where N is the number of these consecutive tokens in the N-gram (examples follow).\n",
        "\n",
        "Consider the short corpus below:\n",
        "\n",
        "Once upon a time there was a child. The child is sad today. However, the child will go out to play, and the child can not be sad anymore.\n",
        "\n",
        "\n",
        "*   Examples of 2-grams include: (once, upon), (upon, a), (a, time), … ,(sad, anymore), (anymore, .) \n",
        "*   Examples of 3-grams include: (once, upon, a), (., the, child), (to, play, ,), (sad, anymore, .)\n",
        "\n",
        "\n",
        "## Task Ahead\n",
        "\n",
        "Your text generation will consider the probability of a specific token following an N-gram and pick\n",
        "a word/punctuation based on those probabilities. To generate the probabilities, you will parse\n",
        "the corpus and count the different occurrences of a single word after an N-gram. Returning to\n",
        "the short corpus using 2-grams, the 2-gram (once, upon) is followed by “a”. The 2-gram (the,\n",
        "child) is followed once by “is”, once by “will”, and once by “can”. Thus, the next word after “once\n",
        "upon” will be “a” with 100% probability. The next word after “the child” will be one of “is”, “will”,\n",
        "and “can”, each with one-thirds probability."
      ],
      "metadata": {
        "id": "cFBLO5TQU5AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utilities import *"
      ],
      "metadata": {
        "id": "R-ViQSFVsl_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 1\n",
        "\n",
        "`parse_story(filename : str) -> list`\n",
        "\n",
        "Returns an ordered list of words with bad characters processed (removed) from the text in the file given by `file_name`."
      ],
      "metadata": {
        "id": "I9OkCNlBYDxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_story(filename):\n",
        "  storyfile = open(filename, 'r')\n",
        "  content = storyfile.read()\n",
        "  content = content.lower()\n",
        "  output = []\n",
        "  word = ''\n",
        "\n",
        "  for i in range(len(content)):\n",
        "    if content[i] in [\" \", '\\n', '\\t'] and word != '':\n",
        "      if word.capitalize() in ALWAYS_CAPITALIZE:\n",
        "        word = word.capitalize()\n",
        "      output.append(word)\n",
        "      word = ''\n",
        "  \n",
        "    elif content[i] in VALID_PUNCTUATION:\n",
        "      if word != '':\n",
        "        if word.capitalize() in ALWAYS_CAPITALIZE:\n",
        "          word = word.capitalize()\n",
        "        output.append(word)\n",
        "      output.append(content[i])\n",
        "      word = ''\n",
        "\n",
        "    elif content[i] not in (BAD_CHARS) and content[i] not in [\" \", '\\n', '\\t']:\n",
        "      word = word + content[i]\n",
        "\n",
        "  storyfile.close()\n",
        "  return output"
      ],
      "metadata": {
        "id": "f3JuqkyNYDIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = parse_story('308.txt')"
      ],
      "metadata": {
        "id": "bjiXU4x58DHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 2\n",
        "\n",
        "`get_prob_from_count(counts : list) -> list`\n",
        "\n",
        "Return a list of probabilities derived from counts. Counts is a list of counts of occurrences of a token after the previous n-gram. You should not round the probabilities."
      ],
      "metadata": {
        "id": "VDQ4mpDhYg47"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmGAX8wVUmm9"
      },
      "outputs": [],
      "source": [
        "def get_prob_from_count(counts):\n",
        "  summation = sum(counts)\n",
        "  output = []\n",
        "\n",
        "  for element in counts:\n",
        "    output.append(element/summation)\n",
        "  \n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_prob_from_count([10,20,30,40,50])"
      ],
      "metadata": {
        "id": "gBkS5XL496QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 3\n",
        "\n",
        "`build_ngram_counts(words : list, n : int) -> dict`\n",
        "\n",
        "Return a dictionary of N-grams (where N=`n`) and the counts of the words in the list `words` that follow the Ngram. The key of the dictionary will be the N-gram in a tuple. The corresponding value will be a list containing two lists. The first list contains the words and the second list contains the corresponding counts.\n",
        "\n",
        "Use parse_story function to test"
      ],
      "metadata": {
        "id": "8GrHDrtyYxP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_counts(words, n):\n",
        "  ngram_dict = {}\n",
        "  for i in range(len(words) - n):\n",
        "\n",
        "    if tuple(words[i : i + n]) not in ngram_dict:\n",
        "      ngram_dict[tuple(words[i : i + n])] = [ [words[i+n]], [1] ]\n",
        "\n",
        "    else:\n",
        "      collection = ngram_dict[tuple(words[i : i + n])]\n",
        "      \n",
        "      if words[i+n] in collection[0]:\n",
        "        index = collection[0].index(words[i+n])\n",
        "        collection[1][index] += 1\n",
        "        ngram_dict[tuple(words[i : i + n])] = collection\n",
        "      \n",
        "      else:\n",
        "        collection[0].append(words[i+n])\n",
        "        collection[1].append(1)\n",
        "        ngram_dict[tuple(words[i : i + n])] = collection\n",
        "    \n",
        "  return ngram_dict"
      ],
      "metadata": {
        "id": "C1tYTrChYxli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = build_ngram_counts(words, 2)"
      ],
      "metadata": {
        "id": "NjWx3G0EKGch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 4\n",
        "\n",
        "`prune_ngram_counts(counts : dict, prune_len : int) -> dict`\n",
        "\n",
        "Return a dictionary of N-grams and counts of words with lower frequency (i.e. occurring less\n",
        "often) words removed. You will prune the words based on their counts, keeping the\n",
        "prune_len highest frequency words. In case of a tie (for example, if `prune_len` was 5 and the 5th and 6th most frequent words had the same frequency), then keep all words that are in the tie (e.g. keep both the 5th and 6th words).\n",
        "\n",
        "Use previous functions here."
      ],
      "metadata": {
        "id": "8A831Wp6bRBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prune_ngram_counts(counts, prune_len):\n",
        "\t'''\n",
        "\t(dict, int) -> (dict)\n",
        "\tReturn a dictionary of N-grams and counts of words with lower frequency (i.e. occurring less often) words removed. \n",
        "\t>>> ngram_counts= {(‘i’, ‘love’): [[‘js’, ‘py3’, ‘c’, ‘no’], [20, 20, 10, 2]],(‘u’, ‘r’): [[‘cool’, ‘nice’, ‘lit’, 'kind’], [8, 7, 5, 5]],('toronto’, ‘is’): [[‘six’, ‘drake’], [2, 3]]}\n",
        "\t>>> prune_ngram_counts(ngram_counts, 3)\n",
        "\t{(‘i’, ‘love’): [[‘js’, ‘py3’, ‘c’], [20, 20, 10]],(‘u’, ‘r’): [[‘cool’, ‘nice’, ‘lit’, 'kind’], [8, 7, 5, 5]],('toronto’, ‘is’): [[‘six’, ‘drake’],[2, 3]]}\n",
        "\t'''\n",
        "\tfor ngram, lists in counts.items():\n",
        "\t\tif len(lists[0]) > prune_len:\n",
        "\t\t\tmin_count = lists[1][1]\n",
        "\t\t\tfor j in lists[1]:\n",
        "\t\t\t\tif j < min_count:\n",
        "\t\t\t\t\tmin_count = j\n",
        "\t\t\tif lists[1].count(min_count) <= (len(lists[0]) - prune_len):\n",
        "\t\t\t\tfor k in range(lists[1].count(min_count)):\n",
        "\t\t\t\t\tind = lists[1].index(min_count)\n",
        "\t\t\t\t\tlists[1].remove(min_count)\n",
        "\t\t\t\t\tlists[0].pop(ind)\n",
        "\t\t\t\tif len(lists[0]) > prune_len:\n",
        "\t\t\t\t\tprune_ngram_counts({ngram : lists}, prune_len)\n",
        "\t\t\telse:\n",
        "\t\t\t\tcounts.update({ngram : lists})\n",
        "\treturn counts"
      ],
      "metadata": {
        "id": "LkDMP3Nfb9J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 5\n",
        "\n",
        "`probify_ngram_counts(counts : dict) -> dict`\n",
        "\n",
        "Take a dictionary of N-grams and counts and convert the counts to probabilities. The probability of each word is defined as the observed count divided by the total count of all words."
      ],
      "metadata": {
        "id": "JmFfeKV_b9ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probify_ngram_counts(counts):\n",
        "  for ngram, lists in counts.items():\n",
        "    lists[1] = get_prob_from_count(lists[1])\n",
        "    counts[ngram] = lists\n",
        "  return counts"
      ],
      "metadata": {
        "id": "Yyz1_AQJdrA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probify_ngram_counts(counts)"
      ],
      "metadata": {
        "id": "ecddwaIpRLtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 6\n",
        "\n",
        "`build_ngram_model(words, n)`\n",
        "\n",
        "Create and return a dictionary of the format given above in probify_ngram_counts. This dictionary is your final model that will be used to auto-generate text. For your final model, keep the 15 most likely words that follow an N-gram. \n",
        "\n",
        "_Challenge_: Moreover, for each N-gram, the corresponding next words should appear in descending order of probability."
      ],
      "metadata": {
        "id": "yq0i-ynZdrYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_model(words, n):\n",
        "\t'''\n",
        "\t(list, int) -> (dict)\n",
        "\t\n",
        "\t'''\n",
        "\ttemp1 = {}\n",
        "\ttemp1.update(build_ngram_counts(words, n))\n",
        "\ttemp1.update(prune_ngram_counts(temp1, 15))\n",
        " \n",
        "  # Challenge Part=============================\n",
        "\tfor ngram, lists in temp1.items():\n",
        "\t\tlists[1], lists[0] = zip(*sorted(zip(lists[1], lists[0]), reverse = True))\n",
        "\t\tlists[1] = list(lists[1])\n",
        "\t\tlists[0] = list(lists[0])\n",
        "  # ===========================================\n",
        "\n",
        "\ttemp1.update(probify_ngram_counts(temp1))\n",
        "\treturn temp1"
      ],
      "metadata": {
        "id": "oUN0jiETeiM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 7\n",
        "\n",
        "`gen_bot_list(ngram_model, seed, num_tokens=0)`\n",
        "\n",
        "Returns a randomly generated list of tokens (strings) that starts with the N tokens in seed, selecting all subsequent tokens using gen_next_token. The list ends when any of the following happens:\n",
        "\n",
        "\n",
        "*   List contains num_tokens tokens, including repetitions. In case seed is longer than num_tokens, the returned list should contain the first num_tokens tokens of the seed.\n",
        "*   Any of the assumptions of gen_next_token is violated. I.e. if either an N-gram is not\n",
        "in the model, or if an N-gram has no tokens that follow it."
      ],
      "metadata": {
        "id": "OUKDvAtleil_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_bot_list(ngram_model, seed, num_tokens = 0):\n",
        "\t'''\n",
        "\t(dict, tuple, int) -> (list)\n",
        "\t\n",
        "\t'''\n",
        "\tif num_tokens == 0:\n",
        "\t\treturn []\n",
        "\telse:\n",
        "\t\tif len(seed) >= num_tokens:\n",
        "\t\t\treturn list(seed[ : num_tokens])\n",
        "\t\telse:\n",
        "\t\t\tif seed not in ngram_model:\n",
        "\t\t\t\treturn list(seed)\n",
        "\t\t\telse:\n",
        "\t\t\t\tsentence = list(seed)\n",
        "\t\t\t\tfor i in range(num_tokens - len(seed)):\n",
        "\t\t\t\t\tif tuple(sentence[i : i + len(seed)]) in ngram_model and len(sentence) < num_tokens:\n",
        "\t\t\t\t\t\tword = gen_next_token(tuple(sentence[i : i + len(seed)]), ngram_model)\n",
        "\t\t\t\t\t\tsentence.append(word)\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\treturn sentence\n",
        "\t\t\t\treturn sentence"
      ],
      "metadata": {
        "id": "64AdntO2fR4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 8\n",
        "\n",
        "`gen_bot_text(token_list, bad_author)`\n",
        "\n",
        "If bad_author is True, returns the string containing all tokens in token_list, separated by a space. Otherwise, returns this string of text, respecting the following grammar rules:\n",
        "\n",
        "*   There are no spaces before tokens found in `VALID_PUNCTUATION`\n",
        "*   A sentence cannot start with a lower-case letter. The array in `utilities.py` called `END_OF_SENTENCE_PUNCTUATION` tells you how to determine that the previous sentence has finished. (Recall that strings are case-sensitive).\n",
        "*   Words in `ALWAYS_CAPITALIZE` should start with a capital letter"
      ],
      "metadata": {
        "id": "mZgJrrsXfSNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_bot_text(token_list, bad_author):\n",
        "\t'''\n",
        "\t(list, bool) -> (str)\n",
        "\t\n",
        "\t'''\n",
        "\tstring = ''\n",
        "\tif bad_author:\n",
        "\t\tfor word in token_list:\n",
        "\t\t\tstring += word\n",
        "\t\t\tstring += ' '\n",
        "\t\treturn string\n",
        "\telse:\n",
        "\t\tcaps = False\n",
        "\t\tcapitals = []\n",
        "\t\tfor i in range(len(ALWAYS_CAPITALIZE)):\n",
        "\t\t\tcapitals.append(ALWAYS_CAPITALIZE[i].lower())\n",
        "\t\tfor word in token_list:\n",
        "\t\t\tif len(string) == 0:\n",
        "\t\t\t\tstring += word.capitalize()\n",
        "\t\t\telse:\n",
        "\t\t\t\tif word in VALID_PUNCTUATION:\n",
        "\t\t\t\t\tcaps = False\n",
        "\t\t\t\t\tif word in END_OF_SENTENCE_PUNCTUATION:\n",
        "\t\t\t\t\t\tcaps = True\n",
        "\t\t\t\t\tstring += word\n",
        "\t\t\t\telif word in capitals:\n",
        "\t\t\t\t\tWord = word.capitalize()\n",
        "\t\t\t\t\tstring += ' ' + Word\n",
        "\t\t\t\t\tcaps =False\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif caps:\n",
        "\t\t\t\t\t\tWord = word.capitalize()\n",
        "\t\t\t\t\t\tstring += ' ' + Word\n",
        "\t\t\t\t\t\tcaps = False\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tstring += ' '+ word  \t\n",
        "\t\treturn string"
      ],
      "metadata": {
        "id": "NEEGWAyWgsJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function 9\n",
        "\n",
        "`write_story(file_name, text, title, student_name, author, year)`\n",
        "\n",
        "Writes the text to the file with name file_name. The file should begin with a title “page”, starting with 10 newline chars and ending with 17 newline chars, and having the following content:\n",
        "\n",
        "**title**: year, UNLEASHED\n",
        "\n",
        "**student_name**, inspired by author\n",
        "\n",
        "Copyright year published (year), publisher: GEC press\n",
        "\n",
        "For exact formatting and spacing between characters refer to test_write_story.txt or test_gen_bot_text.txt.\n",
        "\n",
        "You can comfortably read about 90 characters on a line (excluding new line character from the count). Place the largest number of words you can fit on the 90-character line. Do not break words or any other sequences of characters that were not initially separated by white space. Do not begin or end a line in a space character.\n",
        "\n",
        "When GEC press prints the text, it can fit exactly 30 lines on a page. Place a page number as the last line of each page (starting with 1 and the title page is not numbered). The line before the page number should be blank. The last page should also display the page number on the 30th line (no new line character at the end of the file)\n",
        "\n",
        "Every 12 pages is a new chapter. Write chapter headings followed by two newline characters:\n",
        "\n",
        "CHAPTER #\\n\\n\n",
        "\n",
        "starting with CHAPTER 1\\n\\n"
      ],
      "metadata": {
        "id": "LEjowpmygsde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_story(file_name, text, title, student_name, author, year):\n",
        "\t'''\n",
        "\t(string, string, string, string, string, int) -> (file)\n",
        "\t\n",
        "\t'''\n",
        "\ttest = open(file_name, 'w')\n",
        "\tL = text.split()\n",
        "\tcontent = 'CHAPTER 1\\n\\n'\n",
        "\tcount = 0\n",
        "\tcount_line = 2\n",
        "\tcount_page = 0\n",
        "\tcount_chap = 1\n",
        "\tfor word in L:\n",
        "\t\tif content == 'CHAPTER 1\\n\\n':\n",
        "\t\t\tcontent += word\n",
        "\t\t\tcount += len(word)\n",
        "\t\telse:\t\n",
        "\t\t\tcount += len(word) + 1\n",
        "\t\t\tif count > 90:\n",
        "\t\t\t\tcount_line += 1\n",
        "\t\t\t\tif count_line == 28:\n",
        "\t\t\t\t\tcount_line = 0\n",
        "\t\t\t\t\tcount_page += 1\n",
        "\t\t\t\t\tcontent += '\\n\\n' + str(count_page)\n",
        "\t\t\t\tif count_page % 12 == 0 and count_page != 0 and count_line == 0 and word != L[-1]:\n",
        "\t\t\t\t\tcount_chap += 1\n",
        "\t\t\t\t\tcontent += '\\nCHAPTER ' + str(count_chap) + '\\n'\n",
        "\t\t\t\t\tcount_line = 2\n",
        "\t\t\t\tcount = len(word)\n",
        "\t\t\t\tcontent += '\\n' + word\t\n",
        "\t\t\telse: \n",
        "\t\t\t\tcontent += \" \" + word \n",
        "\t\n",
        "\tif count_line != 0:\n",
        "\t\tfor i in range(29 - count_line):\n",
        "\t\t\tcontent += '\\n'\n",
        "\t\tcontent += str(count_page+1) + '\\n'\n",
        "\ttest.write('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'+title+': '+str(year)+', UNLEASHED\\n'+student_name+', inspired by '+author+'\\nCopyright year published ('+str(year)+'), publisher: EngSci press\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'+content)\n",
        "\ttest.close()"
      ],
      "metadata": {
        "id": "is4tBXHwhrA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}